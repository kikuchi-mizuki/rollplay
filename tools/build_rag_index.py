#!/usr/bin/env python3
"""
RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ãƒ„ãƒ¼ãƒ«

ä½¿ã„æ–¹:
    python tools/build_rag_index.py

æ©Ÿèƒ½:
- transcripts/ã‹ã‚‰æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
- é¡§å®¢ç™ºè¨€ã‚’æŠ½å‡º
- ã‚·ãƒ¼ãƒ³åˆ¤å®šï¼ˆæŒ¨æ‹¶/ãƒ’ã‚¢ãƒªãƒ³ã‚°/ææ¡ˆ/ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ³ã‚°ï¼‰
- Embeddingç”Ÿæˆï¼ˆOpenAI text-embedding-3-largeï¼‰
- FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
"""

import os
import sys
import json
from pathlib import Path
from typing import List, Dict
import numpy as np
import faiss
from openai import OpenAI
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
BASE_DIR = Path(__file__).parent.parent
TRANSCRIPTS_DIR = BASE_DIR / 'transcripts'
RAG_INDEX_DIR = BASE_DIR / 'rag_index'
RAG_INDEX_DIR.mkdir(exist_ok=True)


def detect_scene(text: str, position: float) -> str:
    """
    ç™ºè©±å†…å®¹ã¨positionï¼ˆä¼šè©±å…¨ä½“ã®ä½•%ã®ä½ç½®ã‹ï¼‰ã‹ã‚‰ã‚·ãƒ¼ãƒ³ã‚’åˆ¤å®š

    Args:
        text: ç™ºè©±å†…å®¹
        position: 0.0-1.0ã®ä½ç½®ï¼ˆ0=æœ€åˆã€1=æœ€å¾Œï¼‰

    Returns:
        ã‚·ãƒ¼ãƒ³å
    """
    # æŒ¨æ‹¶ï¼ˆæœ€åˆã®10%ï¼‰
    if position < 0.1:
        return 'greeting'

    # ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ³ã‚°ï¼ˆæœ€å¾Œã®20%ï¼‰
    if position > 0.8:
        return 'closing'

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®åˆ¤å®š
    if any(kw in text for kw in ['ã¯ã˜ã‚ã¾ã—ã¦', 'ã‚ˆã‚ã—ã', 'ã”ç´¹ä»‹', 'ãŠåå‰']):
        return 'greeting'

    if any(kw in text for kw in ['ã‚ã‚ŠãŒã¨ã†', 'ãã‚Œã§ã¯', 'ã‚ˆã‚ã—ããŠé¡˜ã„', 'ä»Šå¾Œ']):
        return 'closing'

    if any(kw in text for kw in ['ææ¡ˆ', 'ãƒ—ãƒ©ãƒ³', 'ã‚µãƒ¼ãƒ“ã‚¹', 'ã“ã¡ã‚‰', 'ä¾‹ãˆã°', 'ã”è¦§']):
        return 'proposal'

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒ‹ãƒ¼ã‚ºåˆ†æ
    return 'needs_analysis'


def extract_topics(text: str) -> List[str]:
    """
    ç™ºè©±å†…å®¹ã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º

    Args:
        text: ç™ºè©±å†…å®¹

    Returns:
        ãƒˆãƒ”ãƒƒã‚¯ã®ãƒªã‚¹ãƒˆ
    """
    topics = []

    # ãƒˆãƒ”ãƒƒã‚¯è¾æ›¸
    topic_keywords = {
        'budget': ['äºˆç®—', 'è²»ç”¨', 'ä¾¡æ ¼', 'é‡‘é¡', 'ã‚³ã‚¹ãƒˆ'],
        'timeline': ['æœŸé–“', 'ã„ã¤', 'ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«', 'ç´æœŸ', 'æ™‚é–“'],
        'examples': ['äº‹ä¾‹', 'å®Ÿç¸¾', 'ä»–ç¤¾', 'ä¾‹', 'ã‚±ãƒ¼ã‚¹'],
        'features': ['æ©Ÿèƒ½', 'ã‚µãƒ¼ãƒ“ã‚¹', 'ãƒ—ãƒ©ãƒ³', 'ã§ãã‚‹', 'ã§ãã¾ã™'],
        'concerns': ['ä¸å®‰', 'å¿ƒé…', 'æ‡¸å¿µ', 'æ‚©ã¿', 'å›°ã£ã¦'],
        'social_media': ['SNS', 'ã‚¤ãƒ³ã‚¹ã‚¿', 'TikTok', 'Twitter', 'Facebook'],
        'video': ['å‹•ç”»', 'ãƒ“ãƒ‡ã‚ª', 'æ˜ åƒ', 'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„'],
        'results': ['åŠ¹æœ', 'æˆæœ', 'å®Ÿç¸¾', 'æ•°å­—', 'åå¿œ'],
    }

    for topic, keywords in topic_keywords.items():
        if any(kw in text for kw in keywords):
            topics.append(topic)

    return topics if topics else ['general']


def generate_embedding(text: str) -> List[float]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã®Embeddingã‚’ç”Ÿæˆ

    Args:
        text: ãƒ†ã‚­ã‚¹ãƒˆ

    Returns:
        Embeddingãƒ™ã‚¯ãƒˆãƒ«
    """
    try:
        response = client.embeddings.create(
            model="text-embedding-3-large",
            input=text
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"âŒ Embeddingç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None


def detect_scenario_from_filename(filename: str) -> str:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚·ãƒŠãƒªã‚ªIDã‚’åˆ¤å®š

    å‘½åè¦å‰‡:
    - meeting_1st_XXX.* â†’ meeting_1st
    - meeting_1_5th_XXX.* â†’ meeting_1_5th
    - meeting_2nd_XXX.* â†’ meeting_2nd
    - meeting_3rd_XXX.* â†’ meeting_3rd
    - kickoff_XXX.* â†’ kickoff_meeting
    - upsell_XXX.* â†’ upsell
    - ãã®ä»– â†’ unknown

    Args:
        filename: ãƒ•ã‚¡ã‚¤ãƒ«å

    Returns:
        ã‚·ãƒŠãƒªã‚ªID
    """
    filename_lower = filename.lower()

    # ã‚·ãƒŠãƒªã‚ªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
    scenario_patterns = {
        'meeting_1st': ['meeting_1st', '1æ¬¡é¢è«‡', '1st_meeting', 'first_meeting'],
        'meeting_1_5th': ['meeting_1_5th', '1.5æ¬¡é¢è«‡', '1_5th_meeting'],
        'meeting_2nd': ['meeting_2nd', '2æ¬¡é¢è«‡', '2nd_meeting', 'second_meeting'],
        'meeting_3rd': ['meeting_3rd', '3æ¬¡é¢è«‡', '3rd_meeting', 'third_meeting'],
        'kickoff_meeting': ['kickoff', 'ã‚­ãƒƒã‚¯ã‚ªãƒ•', 'kick_off'],
        'upsell': ['upsell', 'è¿½åŠ å–¶æ¥­', 'additional_sales', 'cross_sell'],
    }

    for scenario_id, patterns in scenario_patterns.items():
        if any(pattern in filename_lower for pattern in patterns):
            return scenario_id

    # ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ unknown
    return 'unknown'


def process_transcript(transcript_path: Path) -> List[Dict]:
    """
    æ–‡å­—èµ·ã“ã—ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰RAGãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º

    Args:
        transcript_path: æ–‡å­—èµ·ã“ã—JSONã®ãƒ‘ã‚¹

    Returns:
        RAGãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
    """
    print(f"\nğŸ“„ å‡¦ç†ä¸­: {transcript_path.name}")

    with open(transcript_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚·ãƒŠãƒªã‚ªIDã‚’åˆ¤å®š
    source_file = data.get('source_file', transcript_path.stem)
    scenario_id = detect_scenario_from_filename(source_file)
    print(f"   ã‚·ãƒŠãƒªã‚ªID: {scenario_id}")

    segments = data.get('segments', [])
    total_segments = len(segments)

    print(f"   ç·ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {total_segments}")

    # é¡§å®¢ç™ºè¨€ã®ã¿æŠ½å‡º
    rag_data = []
    customer_segments = [s for s in segments if s.get('speaker_type') == 'customer']

    print(f"   é¡§å®¢ç™ºè¨€: {len(customer_segments)}ä»¶")

    for i, seg in enumerate(customer_segments):
        text = seg.get('text', '').strip()
        if not text or len(text) < 5:  # çŸ­ã™ãã‚‹ç™ºè¨€ã¯ã‚¹ã‚­ãƒƒãƒ—
            continue

        # ã‚·ãƒ¼ãƒ³åˆ¤å®š
        position = i / len(customer_segments) if customer_segments else 0
        scene = detect_scene(text, position)

        # ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
        topics = extract_topics(text)

        rag_data.append({
            'type': 'customer_response',
            'text': text,
            'scene': scene,
            'topics': topics,
            'scenario_id': scenario_id,  # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰åˆ¤å®šã—ãŸã‚·ãƒŠãƒªã‚ªID
            'source_file': data.get('source_file'),
        })

    print(f"âœ… {len(rag_data)}ä»¶ã®RAGãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º")

    return rag_data


def build_faiss_index(rag_data: List[Dict]) -> tuple:
    """
    FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰

    Args:
        rag_data: RAGãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ

    Returns:
        (index, metadata)
    """
    print(f"\nğŸ”§ FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ä¸­...")
    print(f"   ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(rag_data)}")

    # Embeddingç”Ÿæˆ
    embeddings = []
    metadata = []

    for i, item in enumerate(rag_data):
        if (i + 1) % 10 == 0:
            print(f"   é€²æ—: {i + 1}/{len(rag_data)}")

        embedding = generate_embedding(item['text'])
        if embedding:
            embeddings.append(embedding)
            metadata.append(item)

    print(f"âœ… {len(embeddings)}ä»¶ã®Embeddingã‚’ç”Ÿæˆ")

    # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
    if not embeddings:
        print("âŒ ã‚¨ãƒ©ãƒ¼: EmbeddingãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        return None, []

    dimension = len(embeddings[0])
    embeddings_array = np.array(embeddings).astype('float32')

    # IndexFlatL2: L2è·é›¢ã§æ¤œç´¢ï¼ˆæ­£ç¢ºã ãŒé…ã„ã€å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‘ã‘ï¼‰
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings_array)

    print(f"âœ… FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰å®Œäº†")
    print(f"   æ¬¡å…ƒæ•°: {dimension}")
    print(f"   ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä»¶æ•°: {index.ntotal}")

    return index, metadata


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print(f"""
{'='*60}
RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ãƒ„ãƒ¼ãƒ«
{'='*60}
å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {TRANSCRIPTS_DIR}
å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {RAG_INDEX_DIR}
{'='*60}
    """)

    # OpenAI APIã‚­ãƒ¼ã®ç¢ºèª
    if not os.getenv('OPENAI_API_KEY'):
        print("âŒ ã‚¨ãƒ©ãƒ¼: OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        sys.exit(1)

    # æ–‡å­—èµ·ã“ã—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    transcript_files = list(TRANSCRIPTS_DIR.glob('*_transcript.json'))

    if not transcript_files:
        print(f"âš ï¸  {TRANSCRIPTS_DIR} ã«æ–‡å­—èµ·ã“ã—ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        print("   å…ˆã« transcribe_videos.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        sys.exit(0)

    print(f"ğŸ“ {len(transcript_files)}ä»¶ã®æ–‡å­—èµ·ã“ã—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º\n")

    # RAGãƒ‡ãƒ¼ã‚¿æŠ½å‡º
    all_rag_data = []
    for tf in transcript_files:
        rag_data = process_transcript(tf)
        all_rag_data.extend(rag_data)

    if not all_rag_data:
        print("âŒ RAGãƒ‡ãƒ¼ã‚¿ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
        sys.exit(1)

    print(f"\n{'='*60}")
    print(f"ç·RAGãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(all_rag_data)}")
    print(f"{'='*60}")

    # ã‚·ãƒŠãƒªã‚ªåˆ¥é›†è¨ˆ
    from collections import Counter
    scenario_counts = Counter(item['scenario_id'] for item in all_rag_data)
    print("\nğŸ“Š ã‚·ãƒŠãƒªã‚ªåˆ¥ä»¶æ•°:")
    for scenario, count in scenario_counts.items():
        print(f"   {scenario}: {count}ä»¶")

    # ã‚·ãƒ¼ãƒ³åˆ¥é›†è¨ˆ
    scene_counts = Counter(item['scene'] for item in all_rag_data)
    print("\nğŸ“Š ã‚·ãƒ¼ãƒ³åˆ¥ä»¶æ•°:")
    for scene, count in scene_counts.items():
        print(f"   {scene}: {count}ä»¶")

    # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
    index, metadata = build_faiss_index(all_rag_data)

    if not index:
        sys.exit(1)

    # ä¿å­˜
    index_path = RAG_INDEX_DIR / 'sales_patterns.faiss'
    metadata_path = RAG_INDEX_DIR / 'sales_patterns.json'

    faiss.write_index(index, str(index_path))
    print(f"ğŸ’¾ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿å­˜: {index_path}")

    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {metadata_path}")

    print(f"\n{'='*60}")
    print(f"âœ… RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰å®Œäº†ï¼")
    print(f"{'='*60}\n")


if __name__ == '__main__':
    main()
