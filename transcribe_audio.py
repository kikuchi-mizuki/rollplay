#!/usr/bin/env python3
"""
éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Whisper APIã‚’ä½¿ç”¨ã—ã¦ã€è©±è€…åˆ†é›¢ä»˜ãã§æ–‡å­—èµ·ã“ã—ã‚’è¡Œã†
"""
import os
import sys
import json
from datetime import datetime
from openai import OpenAI

# OpenAI APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
api_key = os.getenv('OPENAI_API_KEY')
if not api_key:
    print("âŒ Error: OPENAI_API_KEY environment variable not set")
    sys.exit(1)

client = OpenAI(api_key=api_key)

def transcribe_audio(audio_file_path: str, output_dir: str = "transcripts") -> dict:
    """
    éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—

    Args:
        audio_file_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

    Returns:
        æ–‡å­—èµ·ã“ã—çµæœã®è¾æ›¸
    """
    print(f"ğŸ¤ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—ä¸­: {audio_file_path}")

    # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—
    file_name = os.path.basename(audio_file_path)
    file_name_without_ext = os.path.splitext(file_name)[0]

    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã
    with open(audio_file_path, 'rb') as audio_file:
        # Whisper APIã§æ–‡å­—èµ·ã“ã—ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰
        print("ğŸ“ Whisper APIå®Ÿè¡Œä¸­...")
        transcript = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            language="ja",
            response_format="verbose_json"
        )

    print(f"âœ… æ–‡å­—èµ·ã“ã—å®Œäº†: {len(transcript.text)}æ–‡å­—")

    # çµæœã‚’æ§‹é€ åŒ–
    result = {
        "source_file": file_name,
        "processed_at": datetime.now().isoformat(),
        "duration": transcript.duration,
        "language": transcript.language,
        "full_text": transcript.text,
        "segments": []
    }

    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ï¼ˆè©±è€…ã‚’æ¨å®šï¼‰
    print("ğŸ‘¥ è©±è€…åˆ†é›¢å‡¦ç†ä¸­...")
    segments = transcript.segments if hasattr(transcript, 'segments') else []

    for i, segment in enumerate(segments):
        # ç°¡æ˜“çš„ãªè©±è€…æ¨å®šï¼ˆäº¤äº’ã«å–¶æ¥­/é¡§å®¢ã‚’å‰²ã‚Šå½“ã¦ï¼‰
        # ã‚ˆã‚Šé«˜åº¦ãªè©±è€…åˆ†é›¢ã«ã¯pyannoteãªã©ã‚’ä½¿ç”¨
        speaker_type = "sales" if i % 2 == 0 else "customer"
        speaker = "å–¶æ¥­" if speaker_type == "sales" else "é¡§å®¢"

        # segmentãŒdictã‹objectã‹ã‚’åˆ¤å®š
        if isinstance(segment, dict):
            text = segment.get('text', '').strip()
            start = segment.get('start', 0)
            end = segment.get('end', 0)
        else:
            text = segment.text.strip()
            start = segment.start
            end = segment.end

        result["segments"].append({
            "speaker": speaker,
            "speaker_type": speaker_type,
            "text": text,
            "start": start,
            "end": end
        })

    print(f"âœ… ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‡¦ç†å®Œäº†: {len(result['segments'])}å€‹")

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs(output_dir, exist_ok=True)

    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
    output_path = os.path.join(output_dir, f"{file_name_without_ext}_transcript.json")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ æ–‡å­—èµ·ã“ã—çµæœã‚’ä¿å­˜: {output_path}")

    return result

def main():
    if len(sys.argv) < 2:
        print("Usage: python transcribe_audio.py <audio_file_path>")
        sys.exit(1)

    audio_file_path = sys.argv[1]

    if not os.path.exists(audio_file_path):
        print(f"âŒ Error: File not found: {audio_file_path}")
        sys.exit(1)

    # æ–‡å­—èµ·ã“ã—å®Ÿè¡Œ
    result = transcribe_audio(audio_file_path)

    print("\n" + "="*60)
    print("ğŸ“Š æ–‡å­—èµ·ã“ã—çµ±è¨ˆ")
    print("="*60)
    print(f"ãƒ•ã‚¡ã‚¤ãƒ«: {result['source_file']}")
    print(f"æ™‚é•·: {result['duration']:.1f}ç§’")
    print(f"æ–‡å­—æ•°: {len(result['full_text'])}æ–‡å­—")
    print(f"ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {len(result['segments'])}å€‹")
    print("="*60)

if __name__ == "__main__":
    main()
