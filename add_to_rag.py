#!/usr/bin/env python3
"""
æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã‚’RAGãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import os
import sys
import json
import numpy as np
from openai import OpenAI

try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    print("âŒ Error: faiss-cpu not installed. Run: pip install faiss-cpu")
    sys.exit(1)

# OpenAI APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
api_key = os.getenv('OPENAI_API_KEY')
if not api_key:
    print("âŒ Error: OPENAI_API_KEY environment variable not set")
    sys.exit(1)

client = OpenAI(api_key=api_key)

# RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ‘ã‚¹
RAG_INDEX_DIR = "rag_index"
RAG_INDEX_PATH = os.path.join(RAG_INDEX_DIR, 'sales_patterns.faiss')
RAG_METADATA_PATH = os.path.join(RAG_INDEX_DIR, 'sales_patterns.json')

def load_transcript(transcript_path: str) -> dict:
    """æ–‡å­—èµ·ã“ã—JSONã‚’èª­ã¿è¾¼ã‚€"""
    with open(transcript_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def extract_scenario_id(filename: str) -> str:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚·ãƒŠãƒªã‚ªIDã‚’æŠ½å‡º

    Args:
        filename: ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆä¾‹: meeting_1st_001_compressed.mp3ï¼‰

    Returns:
        ã‚·ãƒŠãƒªã‚ªIDï¼ˆä¾‹: meeting_1stï¼‰
    """
    import re

    # meeting_1st, meeting_1_5th, meeting_2nd, meeting_3rd, kickoff_meeting, upsell ãªã©ã‚’æŠ½å‡º
    patterns = [
        r'(meeting_1st)',
        r'(meeting_1_5th)',
        r'(meeting_2nd)',
        r'(meeting_3rd)',
        r'(kickoff_meeting)',
        r'(upsell)',
    ]

    for pattern in patterns:
        match = re.search(pattern, filename)
        if match:
            return match.group(1)

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ•ã‚¡ã‚¤ãƒ«åã®æœ€åˆã®éƒ¨åˆ†ã‚’ä½¿ç”¨
    return filename.split('_')[0]

def create_chunks(transcript: dict, chunk_size: int = 5) -> list:
    """
    ä¼šè©±ã‚’ãƒãƒ£ãƒ³ã‚¯åŒ–ã™ã‚‹

    Args:
        transcript: æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿
        chunk_size: 1ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚Šã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°

    Returns:
        ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
    """
    chunks = []
    segments = transcript['segments']

    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚·ãƒŠãƒªã‚ªIDã‚’æŠ½å‡º
    scenario_id = extract_scenario_id(transcript['source_file'])

    for i in range(0, len(segments), chunk_size):
        chunk_segments = segments[i:i + chunk_size]

        # ãƒãƒ£ãƒ³ã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
        chunk_text = "\n".join([
            f"{seg['speaker']}: {seg['text']}"
            for seg in chunk_segments
        ])

        # ãƒãƒ£ãƒ³ã‚¯ã®è©±è€…ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®šï¼ˆæœ€åˆã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŸºæº–ï¼‰
        speaker_type = chunk_segments[0]['speaker_type']

        chunks.append({
            "text": chunk_text,
            "speaker_type": speaker_type,
            "scenario_id": scenario_id,  # ã‚·ãƒŠãƒªã‚ªIDã‚’è¿½åŠ 
            "source_file": transcript['source_file'],
            "start_time": chunk_segments[0]['start'],
            "end_time": chunk_segments[-1]['end'],
            "segment_count": len(chunk_segments)
        })

    return chunks

def generate_embeddings(texts: list) -> np.ndarray:
    """
    OpenAI Embedding APIã§ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ

    Args:
        texts: ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ

    Returns:
        Embeddingãƒ™ã‚¯ãƒˆãƒ«ã®é…åˆ—
    """
    print(f"ğŸ”¢ Embeddingç”Ÿæˆä¸­: {len(texts)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯")

    embeddings = []
    batch_size = 100  # OpenAI APIã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        print(f"  ãƒãƒƒãƒ {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}")

        response = client.embeddings.create(
            model="text-embedding-3-large",
            input=batch
        )

        batch_embeddings = [item.embedding for item in response.data]
        embeddings.extend(batch_embeddings)

    print(f"âœ… Embeddingç”Ÿæˆå®Œäº†")
    return np.array(embeddings, dtype='float32')

def load_or_create_index(dimension: int = 3072):
    """æ—¢å­˜ã®FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã‚€ã‹ã€æ–°è¦ä½œæˆ"""
    os.makedirs(RAG_INDEX_DIR, exist_ok=True)

    if os.path.exists(RAG_INDEX_PATH) and os.path.exists(RAG_METADATA_PATH):
        print(f"ğŸ“‚ æ—¢å­˜ã®RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿: {RAG_INDEX_PATH}")
        index = faiss.read_index(RAG_INDEX_PATH)

        with open(RAG_METADATA_PATH, 'r', encoding='utf-8') as f:
            metadata = json.load(f)

        print(f"   æ—¢å­˜ãƒ‡ãƒ¼ã‚¿æ•°: {len(metadata)}ä»¶")
        return index, metadata
    else:
        print(f"ğŸ†• æ–°è¦FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ")
        index = faiss.IndexFlatL2(dimension)
        metadata = []
        return index, metadata

def add_to_rag(transcript_path: str):
    """
    æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã‚’RAGã«è¿½åŠ 

    Args:
        transcript_path: æ–‡å­—èµ·ã“ã—JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    """
    print("="*60)
    print("ğŸ“š RAGãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ ")
    print("="*60)

    # 1. æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    print(f"ğŸ“„ æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿: {transcript_path}")
    transcript = load_transcript(transcript_path)

    # 2. ãƒãƒ£ãƒ³ã‚¯åŒ–
    print(f"âœ‚ï¸  ãƒãƒ£ãƒ³ã‚¯åŒ–ä¸­...")
    chunks = create_chunks(transcript, chunk_size=5)
    print(f"âœ… {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ç”Ÿæˆ")

    # 3. Embeddingç”Ÿæˆ
    texts = [chunk['text'] for chunk in chunks]
    embeddings = generate_embeddings(texts)

    # 4. æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿
    index, metadata = load_or_create_index(dimension=embeddings.shape[1])

    # 5. æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    print(f"ğŸ“¥ FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ ä¸­...")
    index.add(embeddings)
    metadata.extend(chunks)

    print(f"âœ… è¿½åŠ å®Œäº†: {len(chunks)}ä»¶")
    print(f"   ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(metadata)}ä»¶")

    # 6. ä¿å­˜
    print(f"ğŸ’¾ RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜ä¸­...")
    faiss.write_index(index, RAG_INDEX_PATH)

    with open(RAG_METADATA_PATH, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    print(f"âœ… ä¿å­˜å®Œäº†")
    print("="*60)

    # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    print("\nğŸ“Š RAGãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆ")
    print("="*60)
    print(f"ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(metadata)}ä»¶")
    print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚µã‚¤ã‚º: {index.ntotal}ä»¶")
    print(f"ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {embeddings.shape[1]}")

    # ã‚½ãƒ¼ã‚¹åˆ¥ã®é›†è¨ˆ
    sources = {}
    for item in metadata:
        source = item.get('source', item.get('source_file', 'unknown'))
        sources[source] = sources.get(source, 0) + 1

    print(f"\nã‚½ãƒ¼ã‚¹åˆ¥ãƒ‡ãƒ¼ã‚¿æ•°:")
    for source, count in sources.items():
        print(f"  - {source}: {count}ä»¶")
    print("="*60)

def main():
    if len(sys.argv) < 2:
        print("Usage: python add_to_rag.py <transcript_json_path>")
        sys.exit(1)

    transcript_path = sys.argv[1]

    if not os.path.exists(transcript_path):
        print(f"âŒ Error: File not found: {transcript_path}")
        sys.exit(1)

    add_to_rag(transcript_path)

if __name__ == "__main__":
    main()
